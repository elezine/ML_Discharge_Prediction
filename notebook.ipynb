{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *âœ­Ëšï½¥ï¾Ÿâœ§*ï½¥ï¾Ÿ ğ“¬ğ“ªğ“·ğ“ªğ“­ğ“²ğ“ªğ“· ğ“¼ğ“±ğ“²ğ“®ğ“µğ“­ ğ“­ğ“²ğ“¼ğ“¬ğ“±ğ“ªğ“»ğ“°ğ“® ğ“¹ğ“»ğ“®ğ“­ğ“²ğ“¬ğ“½ğ“²ğ“¸ğ“· *âœ­Ëšï½¥ï¾Ÿâœ§*ï½¥ï¾Ÿ*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking notes as I go along..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to use a GCP VM for this project, just for practice. Set up one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open('misc_media/gcp_vm_0.png')\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume I need a GPU for the ANN. Not sure exactly because I've never actually used one, but why not? However, there's the obvious price associaated with this... looks like it's about 0.70 per hour... Not the cheapest, but not the most expensive either (lol throwback to when I tried to use the ultra high memory VM and it cost 150 in a day. lesson learned, never again...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I set it up with Ubuntu OS and 10 GB persistent disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then followed my tutorial for setting up an Ubuntu desktop... not really necessary (suppose everything could be done from command line) but maybe kind of convenient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a reminder, to ssh into active VM instance via cmd type:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ gcloud compute ssh hydro-ann --zone us-central1-a --ssh-flag \"-L 5901:localhost:5901\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then you can access via VNC at 5901"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great okay now that that's done... what is this actual project going to be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's establish what data I'm using for this project:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - ECCC discharge data\n",
    " (https://wateroffice.ec.gc.ca/mainmenu/real_time_data_index_e.html)\n",
    " <br>\n",
    " This is real-time and historic discharge data for all the ECCC stations in Canada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ECCC weather station data (https://climate.weather.gc.ca/historical_data/search_historic_data_e.html)\n",
    "<br>\n",
    "This is climate data for ECCC weather stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So obviously I cannot do this for all of the stations in Canada... probably makes sense to just choose a few stations to train and test on. And then can possibly expand to other stations to see if model is broadly applicable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have stations with nearby weather stations. We could do two different model trainings:\n",
    "\n",
    "1. train using discharge stations and close by weather stations\n",
    "2. train using discharge stations and ERA5 reanalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we could test to see how each performed. Does the ERA5 reanalysis do okay in comparison to the close by weather stations? Then, maybe this gives reason to believe resutls for stations without close-by weather stations. Don't know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless, I think I like the idea of choosing stations only with nearby weather stations for training. That way I can turn this variable on/off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what stations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- INDIN RIVER ABOVE CHALCO LAKE - (07SA004) and INDIN RIVER - (10757) - (data until 2004); about 0.25 km from each other; DATA EXISTS\n",
    "- BAKER CREEK AT OUTLET OF LOWER MARTIN LAKE - (07SB013) and YELLOWKNIFE-HENDERSON - (45467) - (data until 2020); about 7 km from each other; DATA EXISTS\n",
    "- HAY RIVER NEAR HAY RIVER - (07OB001) and HAY RIVER A - (1664) - (data until 2014); about 11 km from each other; DATA EXISTS\n",
    "- HANBURY RIVER ABOVE HOARE LAKE - (06JB001) and HANBURY RIVER - (10897) - (data until 2020); about 1.6 km away from each other; DATA EXISTS\n",
    "- SNARE RIVER BELOW GHOST RIVER - (07SA002) and INDIN RIVER - (10757) - (data until 2004); about 50 km away from each other; DATA EXISTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ñ ÑĞºÑƒÑ‡Ğ°Ñ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anyways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now I have 5 stations with nearby weather stations to use and varying time periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 07SA004 ... done\n",
    "- 07SB013 ... done\n",
    "- 07OB001 ... done\n",
    "- 06JB001 ... done\n",
    "- 07SA002 ... done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 10757 ... done 1997-2002\n",
    "- 45467 ... done 2013-2018\n",
    "- 1664 ... done 2009-2014\n",
    "- 10897 ... done 2013-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** note: i might need to download more years for that weather station data if it doesn't overlap well with the discharge data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up the data and preliminary visualizations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Daily Discharge (m3/s) (PARAM = 1) and Daily Water Level (m) (PARAM = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indin = pd.read_csv(r'data/stream_flow/07SA004_daily.csv', sep='\\\\t')\n",
    "baker = pd.read_csv(r'data/stream_flow/07SB013_daily.csv', sep= '\\t')\n",
    "hay = pd.read_csv(r'data/stream_flow/07OB001_daily.csv', sep= '\\t')\n",
    "hanbury = pd.read_csv(r'data/stream_flow/06JB001_daily.csv', sep= '\\t')\n",
    "snare = pd.read_csv(r'data/stream_flow/07SA002_daily.csv', sep= '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indin_flow = indin.loc[indin['PARAM'] == 1]\n",
    "indin_lev = indin.loc[indin['PARAM'] == 2]\n",
    "\n",
    "baker_flow = baker.loc[baker['PARAM'] == 1]\n",
    "baker_lev = baker.loc[baker['PARAM'] == 2]\n",
    "\n",
    "hay_flow = hay.loc[hay['PARAM'] == 1]\n",
    "hay_lev = hay.loc[hay['PARAM'] == 2]\n",
    "\n",
    "hanbury_flow = hanbury.loc[hanbury['PARAM'] == 1]\n",
    "hanbury_lev = hanbury.loc[hanbury['PARAM'] == 2]\n",
    "\n",
    "snare_flow = snare.loc[snare['PARAM'] == 1]\n",
    "snare_lev = snare.loc[snare['PARAM'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = hay_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.columns = ['station', 'param', 'date', 'flow', 'sym']\n",
    "\n",
    "flow['datetime'] = pd.to_datetime(flow['date'])\n",
    "\n",
    "#hanbury_holdout = hanbury_flow[(hanbury_flow['datetime'] > '12-31-2017') & (hanbury_flow['datetime'] < '01-01-2019')]\n",
    "#hanbury_holdout = hanbury_holdout.set_index('datetime')\n",
    "#level = hanbury_lev[(hanbury_lev['datetime'] > '12-31-2017') & (hanbury_lev['datetime'] < '01-01-2019')]\n",
    "#level = level.set_index('datetime')\n",
    "#hanbury_holdout['level'] = level['level']\n",
    "\n",
    "#snare_holdout['station'] = '07SA004'\n",
    "\n",
    "#hanbury_holdout = hanbury_holdout.drop(['param', 'sym'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hanbury_holdout.to_csv('hanbury_holdout.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test/validation set is an entire year of the 5-year datasets.\n",
    "I will just split the year into four \"seasons\", JFM, AMJ, JAS, OND. We will randomly select half of a season for validation and half of a season for testing from a random 4/5 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = flow[(flow['datetime'] > '12-31-2008') & (flow['datetime'] < '01-01-2014')]\n",
    "flow = flow.set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2009, 2010, 2011, 2012, 2013]\n",
    "year_seas = []\n",
    "chosen_years = []\n",
    "half_seasons_str = [['01-01', '02-16'], ['02-16','04-01'], ['04-01', '05-16'], ['05-16', '07-01'], ['07-01', '08-16'], ['08-16', '10-01'], ['10-01', '11-16'], ['11-16', '12-31']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_seasons = np.arange(0,8,1)\n",
    "for i in range(0,4):\n",
    "    year = np.random.choice(years)\n",
    "    years = np.delete(years, np.where(years == year))\n",
    "    chosen_years.append(year)\n",
    "    \n",
    "    seas_0 = np.random.choice(half_seasons)\n",
    "    seases = []\n",
    "    seases.append(seas_0)\n",
    "    half_seasons = np.delete(half_seasons, np.where(half_seasons == seas_0))\n",
    "    seas_1 = np.random.choice(half_seasons)\n",
    "    half_seasons = np.delete(half_seasons, np.where(half_seasons == seas_1))\n",
    "    seases.append(seas_1)\n",
    "    year_seas.append(seases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chosen_years)\n",
    "print(year_seas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = half_seasons_str[year_seas[0][0]][0] + '-' + str(chosen_years[0])\n",
    "end = half_seasons_str[year_seas[0][0]][1] + '-' + str(chosen_years[0])\n",
    "test = flow[(flow.index >= start) & (flow.index<end)]\n",
    "start = half_seasons_str[year_seas[0][1]][0] + '-' + str(chosen_years[0])\n",
    "end = half_seasons_str[year_seas[0][1]][1] + '-' + str(chosen_years[0])\n",
    "valid = flow[(flow.index >= start) & (flow.index<end)]\n",
    "\n",
    "for i,year in enumerate(chosen_years[1:]):\n",
    "    start = half_seasons_str[year_seas[i][0]][0] + '-' + str(year)\n",
    "    end = half_seasons_str[year_seas[i][0]][1] + '-' + str(year)\n",
    "    test = test.append(flow[(flow.index >= start) & (flow.index<end)])\n",
    "    start = half_seasons_str[year_seas[i][1]][0] + '-' + str(year)\n",
    "    end = half_seasons_str[year_seas[i][1]][1] + '-' + str(year)\n",
    "    valid = valid.append(flow[(flow.index >= start) & (flow.index<end)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(['param', 'sym'], axis=1)\n",
    "test = test.sort_values(by='datetime')\n",
    "test.to_csv('hay_test.csv')\n",
    "\n",
    "valid = valid.drop(['param', 'sym'], axis=1)\n",
    "valid = valid.sort_values(by='datetime')\n",
    "valid.to_csv('hay_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dates = pd.read_csv('hay_test.csv').set_index('datetime')\n",
    "valid_dates = pd.read_csv('hay_valid.csv').set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[np.isin(train.date.values,test_dates.date.values, invert=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['param', 'sym'], axis=1)\n",
    "train = train.sort_values(by='datetime')\n",
    "train.to_csv('hay_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ugh okay now I need the associated weather station data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('hanbury_test.csv')\n",
    "train = pd.read_csv('hanbury_train.csv')\n",
    "valid = pd.read_csv('hanbury_valid.csv')\n",
    "holdout = pd.read_csv('hanbury_holdout.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_0 = pd.read_csv(r'data/weather/10897/10897_2013_daily.csv', sep=',')\n",
    "weather_1 = pd.read_csv(r'data/weather/10897/10897_2014_daily.csv', sep=',')\n",
    "weather_2 = pd.read_csv(r'data/weather/10897/10897_2015_daily.csv', sep=',')\n",
    "weather_3 = pd.read_csv(r'data/weather/10897/10897_2016_daily.csv', sep=',')\n",
    "weather_4 = pd.read_csv(r'data/weather/10897/10897_2017_daily.csv', sep=',')\n",
    "holdout_weather = pd.read_csv(r'data/weather/10897/10897_2018_daily.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_0['datetime'] = pd.to_datetime(weather_0['Date/Time'], format = '%Y-%m-%d')\n",
    "weather_0 = weather_0.set_index('datetime')\n",
    "weather_1['datetime'] = pd.to_datetime(weather_1['Date/Time'], format = '%Y-%m-%d')\n",
    "weather_1 = weather_1.set_index('datetime')\n",
    "weather_2['datetime'] = pd.to_datetime(weather_2['Date/Time'], format = '%Y-%m-%d')\n",
    "weather_2 = weather_2.set_index('datetime')\n",
    "weather_3['datetime'] = pd.to_datetime(weather_3['Date/Time'], format = '%Y-%m-%d')\n",
    "weather_3 = weather_3.set_index('datetime')\n",
    "weather_4['datetime'] = pd.to_datetime(weather_4['Date/Time'], format = '%Y-%m-%d')\n",
    "weather_4 = weather_4.set_index('datetime')\n",
    "\n",
    "holdout_weather['datetime'] = pd.to_datetime(holdout_weather['Date/Time'], format = '%Y-%m-%d')\n",
    "holdout_weather = holdout_weather.set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = weather_0.append(weather_1).append(weather_2).append(weather_3).append(weather_4)\n",
    "#weather = holdout_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(df['datetime'], format = '%Y-%m-%d') \n",
    "df = df.set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['st_max_temp'] = weather['Max Temp (Â°C)']\n",
    "df['st_min_temp'] = weather['Min Temp (Â°C)']\n",
    "df['st_mean_temp'] = weather['Mean Temp (Â°C)']\n",
    "df['st_heat_deg_days'] = weather['Heat Deg Days (Â°C)']\n",
    "df['st_cool_deg_days'] = weather['Cool Deg Days (Â°C)']\n",
    "df['st_total_rain'] = weather['Total Rain (mm)']\n",
    "df['st_total_snow'] = weather['Total Snow (cm)']\n",
    "df['st_total_precip'] = weather['Total Precip (mm)']\n",
    "df['st_total_snow_on_ground'] = weather['Total Precip (mm)']\n",
    "df['st_dir_of_max_gust_10sdeg'] = weather['Dir of Max Gust (10s deg)']\n",
    "df['st_spd_of_max_gust_kmh'] = weather['Spd of Max Gust (km/h)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('hanbury_valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alright now I need to add the ERA5 reanalysis data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b027d314c3e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxarray\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xarray'"
     ]
    }
   ],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coords of weather stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
